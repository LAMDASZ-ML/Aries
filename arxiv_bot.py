import os
import arxiv
import requests
from datetime import datetime, timedelta
import schedule
import time
from dotenv import load_dotenv
from typing import List, Dict
import json
from tqdm import tqdm
# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class ArxivPaperAgent:
    def __init__(self):
        self.webhook_url = os.getenv('WEBHOOK_URL')
        self.deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')
        self.deepseek_url = "https://api.deepseek.com/v1/chat/completions"
    
    def is_relevant_paper(self, title: str, abstract: str) -> bool:
        """ä½¿ç”¨Deepseekåˆ¤æ–­è®ºæ–‡æ˜¯å¦ä¸LLM reasoningç›¸å…³"""
        if 'reasoning' in title.lower() or 'reasoning' in abstract.lower():
            return True
        if 'fast and slow thinking' in title.lower() or 'fast and slow thinking' in abstract.lower():
            return True
        headers = {
            "Authorization": f"Bearer {self.deepseek_api_key}",
            "Content-Type": "application/json"
        }
        
        prompt = f"""è¯·åˆ¤æ–­è¿™ç¯‡è®ºæ–‡æ˜¯å¦ä¸LLMæ¨ç†(reasoning)ã€ç¥ç»ç¬¦å·æ¨ç†ã€é€»è¾‘æ¨ç†ã€æœç´¢ã€MCTSç›¸å…³ã€‚
        
        æ ‡é¢˜: {title}
        æ‘˜è¦: {abstract}
        
        è¯·åªå›ç­”"æ˜¯"æˆ–"å¦"ã€‚å¦‚æœè®ºæ–‡ä¸»è¦ç ”ç©¶LLMçš„æ¨ç†èƒ½åŠ›ã€æ¨ç†æ–¹æ³•ã€é€»è¾‘æ¨ç†ã€ç¥ç»ç¬¦å·æ¨ç†ã€LLM searchã€Inference Scaling Lawã€Fast and Slow Thinkingç­‰ä¸»é¢˜ï¼Œå›ç­”"æ˜¯"ï¼›
        å¦‚æœè®ºæ–‡ä¸»è¦å…³æ³¨å…¶ä»–ä¸»é¢˜ï¼Œå›ç­”"å¦"ã€‚"""
        
        data = {
            "model": "deepseek-chat",
            "messages": [{"role": "user", "content": prompt}]
        }
        
        try:
            response = requests.post(self.deepseek_url, headers=headers, json=data)
            if response.status_code == 200:
                answer = response.json()['choices'][0]['message']['content'].strip().lower()
                return "æ˜¯" in answer
            return False
        except Exception as e:
            print(f"Error in relevance check: {e}")
            return False

    def fetch_papers(self) -> List[Dict]:
        """è·å–æœ€æ–°çš„LLMç›¸å…³è®ºæ–‡å¹¶ç­›é€‰"""
        # è®¾ç½®æœç´¢æŸ¥è¯¢ï¼Œè·å–æ›´å¤šè®ºæ–‡ä»¥ä¾¿ç­›é€‰
        search = arxiv.Search(
            query="cat:cs.AI AND (LLM OR 'Large Language Model Reasoning' OR 'LLM Reasoning' OR 'Neuro-Symbolic' OR 'Fast and Slow Thinking')",
            max_results=100,  # å¢åŠ åˆå§‹è·å–æ•°é‡
            sort_by=arxiv.SortCriterion.SubmittedDate
        )
        # print([result.title for result in search.results()])
        papers = []
        for result in tqdm(search.results()):
            # ä½¿ç”¨Deepseekåˆ¤æ–­è®ºæ–‡ç›¸å…³æ€§
            if self.is_relevant_paper(result.title, result.summary):
                paper = {
                    'title': result.title,
                    'abstract': result.summary,
                    'url': result.entry_id
                }
                papers.append(paper)
                # å½“æ”¶é›†åˆ°è¶³å¤Ÿçš„ç›¸å…³è®ºæ–‡æ—¶åœæ­¢
                if len(papers) >= 10:
                    break
        
        return papers

    def summarize_with_deepseek(self, abstract: str) -> str:
        """ä½¿ç”¨Deepseek APIæ€»ç»“è®ºæ–‡"""
        headers = {
            "Authorization": f"Bearer {self.deepseek_api_key}",
            "Content-Type": "application/json"
        }
        
        prompt = f"è¯·æ ¹æ®æ‘˜è¦ç”¨ä¸€å¥è¯æ€»ç»“è¿™ç¯‡æ–‡ç« çš„æ ¸å¿ƒå†…å®¹: {abstract}"
        
        data = {
            "model": "deepseek-chat",
            "messages": [{"role": "user", "content": prompt}]
        }
        
        response = requests.post(self.deepseek_url, headers=headers, json=data)
        if response.status_code == 200:
            return response.json()['choices'][0]['message']['content']
        return "Failed to generate summary."

    def send_to_feishu(self, summaries: List[Dict]):
        """å‘é€æ¶ˆæ¯åˆ°é£ä¹¦"""
        message = {
            "msg_type": "post",
            "content": {
                "post": {
                    "zh_cn": {
                        "title": f"ä»Šæ—¥LLM Reasoningè®ºæ–‡æ›´æ–° - {datetime.now().strftime('%Y-%m-%d')}",
                        "content": [
                            [{
                                "tag": "text",
                                "text": f"ğŸ“‘ {paper['title']}\n"
                                       f"ğŸ’¡ æ€»ç»“: {paper['summary']}\n"
                                       f"ğŸ”— é“¾æ¥: {paper['url']}\n\n"
                            }] for paper in summaries
                        ]
                    }
                }
            }
        }
        
        response = requests.post(self.webhook_url, json=message)
        return response.status_code == 200

    def run(self):
        """è¿è¡Œä¸»æµç¨‹"""
        papers = self.fetch_papers()
        summaries = []
        
        for paper in papers:
            summary = self.summarize_with_deepseek(paper['abstract'])
            summaries.append({
                'title': paper['title'],
                'summary': summary,
                'url': paper['url']
            })
        
        self.send_to_feishu(summaries)

def main():
    agent = ArxivPaperAgent()
    # agent.run()
    # è®¾ç½®æ¯å¤©æ—©ä¸Š9ç‚¹è¿è¡Œ
    schedule.every().day.at("09:00").do(agent.run)
    
    while True:
        schedule.run_pending()
        time.sleep(60)

if __name__ == "__main__":
    main() 